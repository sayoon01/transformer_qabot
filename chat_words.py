import torch, re
import joblib
from train_qabot_words import TransformerLM

device = "cuda" if torch.cuda.is_available() else "cpu"

# ğŸ”¹ vocab ë¶ˆëŸ¬ì˜¤ê¸°
stoi = joblib.load("stoi.pkl")
itos = joblib.load("itos.pkl")
vocab_size = len(stoi)

encode = lambda s: [stoi.get(w, 0) for w in s.lower().split()]
decode = lambda ids: " ".join([itos[i] for i in ids])

# ğŸ”¹ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model = TransformerLM(vocab_size).to(device)
model.load_state_dict(torch.load("qabot_words.pt", map_location=device))
model.eval()

print("ğŸ’¬ ì±—ë´‡ ì‹œì‘! (quit/exit ì…ë ¥ ì‹œ ì¢…ë£Œ)")

# ğŸ”¹ ëŒ€í™” ë£¨í”„
while True:
    q = input("ë„ˆ: ")
    if q.strip().lower() in ["quit", "exit"]:
        print("ëŒ€í™” ì¢…ë£Œ!")
        break

    # Q: ì§ˆë¬¸ â†’ A: ë‹µë³€ í˜•íƒœë¡œ prompt
    prompt = f"q: {q.lower()} a:"
    x = torch.tensor([encode(prompt)], dtype=torch.long).to(device)

    out = model.generate(x, max_new_tokens=40)
    answer = decode(out[0].tolist())

    # âœ… í›„ì²˜ë¦¬: prompt ë¶€ë¶„ ì œê±° & ë‹¤ìŒ Q: ë‚˜ì˜¤ê¸° ì „ì— ìë¥´ê¸°
    if answer.startswith(prompt):
        answer = answer[len(prompt):].strip()
    answer = re.split(r"q\s*:", answer)[0].strip()

    print("ë´‡:", answer)
