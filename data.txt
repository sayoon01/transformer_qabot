Q: 트랜스포머가 뭐야?
A: 어텐션을 이용해 문맥을 한꺼번에 보는 딥러닝 모델이야.

Q: 트랜스포머의 장점은?
A: 병렬 학습이 가능하고 긴 의존 관계를 잘 학습할 수 있어.

Q: 트랜스포머의 단점은?
A: 계산량이 많고 긴 입력 처리에 비효율적이야.

Q: GPT는 뭐야?
A: 트랜스포머 디코더 기반의 언어 생성 모델이야.

Q: BERT는 뭐야?
A: 트랜스포머 인코더 기반의 언어 이해 모델이야.

Q: 어텐션이 뭐야?
A: 입력 단어들 간의 연관성을 가중치로 계산하는 메커니즘이야.

Q: 자기어텐션(self-attention)은 뭐야?
A: 문장 안의 단어들이 서로 어떤 관계인지 스스로 계산하는 방법이야.

Q: 인코더는 뭐야?
A: 입력 문장을 이해하도록 특징을 추출하는 부분이야.

Q: 디코더는 뭐야?
A: 이해한 정보를 바탕으로 새로운 출력을 생성하는 부분이야.

Q: 포지셔널 인코딩은 왜 필요해?
A: 트랜스포머는 순서를 모르기 때문에 위치 정보를 더해주는 거야.

Q: 멀티헤드 어텐션은 뭐야?
A: 서로 다른 시각에서 여러 번 어텐션을 수행해 더 풍부한 표현을 만드는 거야.

Q: 피드포워드 네트워크는 왜 쓰여?
A: 어텐션으로 얻은 정보를 더 복잡하게 가공하는 단계야.

Q: 레이어 정규화(layer norm)는 뭐야?
A: 학습을 안정적으로 하고 수렴을 빠르게 하는 정규화 기법이야.

Q: 드롭아웃(dropout)은 왜 필요해?
A: 과적합을 방지하고 모델이 일반화되도록 도와줘.

Q: 마스킹(masking)은 언제 필요해?
A: 미래 단어를 보지 않고 다음 단어를 예측해야 할 때 사용해.

Q: 트랜스포머는 RNN과 뭐가 달라?
A: RNN은 순차적으로 처리하지만 트랜스포머는 병렬로 처리해 더 빠르고 효율적이야.

Q: 트랜스포머는 CNN과 뭐가 달라?
A: CNN은 지역적인 특징에 강하고, 트랜스포머는 전체 문맥을 한꺼번에 본다는 점이 달라.

Q: 트랜스포머는 어디에 쓰여?
A: 번역, 요약, 질의응답, 챗봇, 음성 처리 등 다양한 분야에서 쓰여.

Q: 챗GPT는 어떤 모델이야?
A: GPT 시리즈를 기반으로 한 대규모 언어 생성 모델이야.

Q: 파인튜닝(fine-tuning)이 뭐야?
A: 이미 학습된 모델을 특정 작업에 맞게 추가 학습하는 거야.

Q: 프리트레이닝(pre-training)이 뭐야?
A: 대규모 데이터로 일반적인 언어 패턴을 먼저 학습하는 과정이야.
